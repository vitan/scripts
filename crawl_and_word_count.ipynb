{
 "metadata": {
  "name": "crawl_and_word_count"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# Written by Weitao Zhou <zhouwtlord@gmail.com>\n",
      "\n",
      "import urllib2\n",
      "import optparse\n",
      "import Queue\n",
      "import re\n",
      "import threading\n",
      "import time\n",
      "\n",
      "__version__ = '0.1'\n",
      "\n",
      "USAGE = \"%prog [options] <urls_input_file> <target_dir> <thread_num>\"\n",
      "ARGS_MIN_LENGTH = 2\n",
      "VERSION = \"%prog v\" + __version__\n",
      "\n",
      "#RE_PATTERN = r'[\\w]{1,}'\n",
      "RE_PATTERN = r'[a-zA-Z0-9]{1,}'\n",
      "AGENT = \"%s/%s\" % (__version__, __name__)\n",
      "THREAD_NUM = 10\n",
      "\n",
      "\n",
      "class CrawlCount(threading.Thread):\n",
      "    \n",
      "    def __init__(self, url_q, target):\n",
      "        self.url_q = url_q\n",
      "        self.target = target\n",
      "    \n",
      "    def run(self):\n",
      "        queue_non_empty = True\n",
      "        while queue_non_empty:\n",
      "            try:\n",
      "                url = url_q.get(False)\n",
      "                fetcher = Fetcher(url)\n",
      "                fetcher.fetch()\n",
      "                content = fetcher.content \n",
      "                \n",
      "                wordcount = WordCount(content, RE_PATTERN)\n",
      "                wordcount.count()   \n",
      "                \n",
      "                compose = [item.join('=') for item in wordcount.items()]\n",
      "                \n",
      "                with open(os.path.join(self.target, self.url), 'w', 'utf-8') as outf:\n",
      "                    outf.write(compose.join('\\n'))\n",
      "                    outf.save()\n",
      "            except Queue.Empty:\n",
      "                queue_non_empty = False\n",
      "\n",
      "class Fetcher(object):\n",
      "\n",
      "    def __init__(self, url):\n",
      "        self.url = url\n",
      "        self.content = None\n",
      "\n",
      "    def _addHeaders(self, request):\n",
      "        request.add_header(\"User-Agent\", AGENT)\n",
      "\n",
      "    def open(self):\n",
      "        url = self.url\n",
      "        try:\n",
      "            request = urllib2.Request(url)\n",
      "            handle = urllib2.build_opener()\n",
      "        except IOError:\n",
      "            return None\n",
      "        return (request, handle)\n",
      "\n",
      "    def fetch(self):\n",
      "        request, handle = self.open()\n",
      "        self._addHeaders(request)\n",
      "        if handle:\n",
      "            try:\n",
      "                self.content = unicode(handle.open(request).read(), \"utf-8\", errors=\"replace\")\n",
      "            except urllib2.HTTPError, error:\n",
      "                if error.code == 404:\n",
      "                    print >> sys.stderr, \"ERROR: %s -> %s\" % (error, error.url)\n",
      "                else:\n",
      "                    print >> sys.stderr, \"ERROR: %s\" % error\n",
      "            except urllib2.URLError, error:\n",
      "                print >> sys.stderr, \"ERROR: %s\" % error\n",
      "\n",
      "\n",
      "class WordCount(object):\n",
      "    \n",
      "    def __init__(self, text, re_pattern):\n",
      "        self.re_pattern = re_pattern\n",
      "        self.text = text\n",
      "        self.word_count = dict()\n",
      "        \n",
      "    def pre_process(self):\n",
      "        # Pre process the inputted Text:\n",
      "        # 1. remove punctuation; 2. lower case;\n",
      "        # Note: Given interview deadline, here I didn't involve tokenize, stemming, stopwords and other NLP pre-process methods\n",
      "        word_list = self._remove_punctuation()\n",
      "        return self._lower_case(word_list)\n",
      "    \n",
      "    def _remove_punctuation(self):\n",
      "        word_list = re.findall(self.re_pattern, self.text)\n",
      "        return word_list\n",
      "    \n",
      "    def _lower_case(self, word_list):\n",
      "        return [word.lower() for word in word_list]\n",
      "    \n",
      "    def count(self):\n",
      "        word_list = self.pre_process()\n",
      "        for word in word_list:\n",
      "            self.word_count[word] = self.word_count.get(word, 0) + 1\n",
      "\n",
      "def parse_options(args_min_length):\n",
      "    \"\"\"parse_options() -> opts, args\n",
      "\n",
      "    Parse any command-line options given returning both\n",
      "    the parsed options and arguments.\n",
      "    \"\"\"\n",
      "\n",
      "    parser = optparse.OptionParser(usage=USAGE, version=VERSION)\n",
      "\n",
      "\n",
      "    opts, args = parser.parse_args()\n",
      "\n",
      "    if len(args) < args_min_length:\n",
      "        parser.print_help()\n",
      "        raise SystemExit, 1\n",
      "\n",
      "    return opts, args\n",
      "\n",
      "def main():\n",
      "    opts, args = parse_options(ARGS_MIN_LENGTH)\n",
      "    \n",
      "    urls_inf = args[0]\n",
      "    target_dir = args[1]\n",
      "    thread_num = args[3] or THREAD_NUM\n",
      "    \n",
      "    splite = ['=']*20\n",
      "    print splite\n",
      "\n",
      "    # in case of duplicated urls\n",
      "    url_set = set()\n",
      "    q = Queue.Queue()\n",
      "    with open(urls_inf, 'r') as urls:\n",
      "        for line in urls:\n",
      "            url_set.append(line)\n",
      "        \n",
      "        for url in url_set:\n",
      "            q.put(url)\n",
      "    \n",
      "    threads = []\n",
      "    start_time = time.time()\n",
      "    # multiple thread\n",
      "    for i in range(thread_num):\n",
      "        t = CrawlCount(q, target_dir)\n",
      "        t.start()\n",
      "        threads.append(t)\n",
      "    \n",
      "    for t in threads:\n",
      "        t.join()\n",
      "    end_time = time.time()\n",
      "    due = start_time-end_time\n",
      "    \n",
      "    print \"DONE\"\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SystemExit",
       "evalue": "2",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Usage: -c [options] <url>\n",
        "\n",
        "-c: error: no such option: -f\n",
        "To exit: use 'exit', 'quit', or Ctrl-D."
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}