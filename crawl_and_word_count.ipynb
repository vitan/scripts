{
 "metadata": {
  "name": "crawl_and_word_count"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/env python\n",
      "# Written by Weitao Zhou <zhouwtlord@gmail.com>\n",
      "\n",
      "import hashlib\n",
      "import urllib2\n",
      "import optparse\n",
      "import os\n",
      "import Queue\n",
      "import re\n",
      "import socket\n",
      "import sys\n",
      "import threading\n",
      "import time\n",
      "\n",
      "__version__ = '0.1'\n",
      "\n",
      "USAGE = \"%prog [options] <urls_input_file> <target_dir> <thread_num> <thread_delay> <timeout>\"\n",
      "ARGS_MIN_LENGTH = 5\n",
      "VERSION = \"%prog v\" + __version__\n",
      "\n",
      "#RE_PATTERN = r'[\\w]{1,}'\n",
      "RE_PATTERN = r'[a-zA-Z0-9]{1,}'\n",
      "\n",
      "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'}\n",
      "\n",
      "\n",
      "class CrawlCount(threading.Thread):\n",
      "    \n",
      "    def __init__(self, queue, target, delay):\n",
      "        threading.Thread.__init__(self)\n",
      "        self.queue = queue\n",
      "        self.target = target\n",
      "        self.delay = delay\n",
      "    \n",
      "    def run(self):\n",
      "        queue_non_empty = True\n",
      "        while queue_non_empty:\n",
      "            try:\n",
      "                url = self.queue.get(False)\n",
      "                fetcher = Fetcher(url)\n",
      "                if fetcher.fetch() == False:\n",
      "                    # Fetch Failed\n",
      "                    continue\n",
      "                content = fetcher.content \n",
      "                \n",
      "                wordcount = WordCount(content, RE_PATTERN)\n",
      "                wordcount.count()   \n",
      "                \n",
      "                compose = ['%s=%s' % item for item in wordcount.word_count.items()]\n",
      "                \n",
      "                # TODO\n",
      "                # 1. how to avoid duplicated filename caused by duplicated url\n",
      "                # 2. As linux can't accept '/' as filename, hash the url to solve it\n",
      "                filename = hashlib.sha256(url).hexdigest()\n",
      "                with open(os.path.join(self.target, filename), 'w') as outf:\n",
      "                    outf.write('\\n'.join(compose))\n",
      "                \n",
      "                time.sleep(self.delay)\n",
      "            except Queue.Empty:\n",
      "                queue_non_empty = False\n",
      "\n",
      "\n",
      "class Fetcher(object):\n",
      "\n",
      "    def __init__(self, url):\n",
      "        self.url = url\n",
      "        self.content = None\n",
      "\n",
      "    def _addHeaders(self, request):\n",
      "        for key, value in HEADERS.items():\n",
      "            request.add_header(key, value)\n",
      "\n",
      "    def open(self):\n",
      "        url = self.url\n",
      "        try:\n",
      "            print 'Fetching url %s' % url\n",
      "            request = urllib2.Request(url)\n",
      "            handle = urllib2.build_opener()\n",
      "        except IOError:\n",
      "            return None\n",
      "        return (request, handle)\n",
      "\n",
      "    def fetch(self):\n",
      "        request, handle = self.open()\n",
      "        self._addHeaders(request)\n",
      "        if handle:\n",
      "            try:\n",
      "                self.content = unicode(handle.open(request).read(), \"utf-8\", errors=\"replace\")\n",
      "                return True\n",
      "            except urllib2.HTTPError, error:\n",
      "                if error.code == 404:\n",
      "                    print >> sys.stderr, \"ERROR: %s -> %s\" % (error, error.url)\n",
      "                else:\n",
      "                    print >> sys.stderr, \"ERROR: %s\" % error\n",
      "                return False\n",
      "            except urllib2.URLError, error:\n",
      "                print >> sys.stderr, \"ERROR: %s\" % error\n",
      "                return False\n",
      "            except socket.timeout, error:\n",
      "                print >> sys.stderr, \"ERROR: %s\" % error\n",
      "                return False\n",
      "        return False\n",
      "\n",
      "\n",
      "class WordCount(object):\n",
      "    \n",
      "    def __init__(self, text, re_pattern):\n",
      "        self.re_pattern = re_pattern\n",
      "        self.text = text\n",
      "        self.word_count = dict()\n",
      "        \n",
      "    def pre_process(self):\n",
      "        # Pre process the inputted Text:\n",
      "        # 1. remove punctuation; 2. lower case;\n",
      "        # Note: Given interview deadline, here I didn't involve tokenize, stemming, stopwords and other NLP pre-process methods\n",
      "        word_list = self._remove_punctuation()\n",
      "        return self._lower_case(word_list)\n",
      "    \n",
      "    def _remove_punctuation(self):\n",
      "        print 'Removing punctuation'\n",
      "        word_list = re.findall(self.re_pattern, self.text)\n",
      "        return word_list\n",
      "    \n",
      "    def _lower_case(self, word_list):\n",
      "        print 'Lowering case'\n",
      "        return [word.lower() for word in word_list]\n",
      "    \n",
      "    def count(self):\n",
      "        print 'Word Counting'\n",
      "        word_list = self.pre_process()\n",
      "        for word in word_list:\n",
      "            self.word_count[word] = self.word_count.get(word, 0) + 1\n",
      "\n",
      "def parse_options(args_min_length):\n",
      "    \"\"\"parse_options() -> opts, args\n",
      "\n",
      "    Parse any command-line options given returning both\n",
      "    the parsed options and arguments.\n",
      "    \"\"\"\n",
      "\n",
      "    parser = optparse.OptionParser(usage=USAGE, version=VERSION)\n",
      "\n",
      "\n",
      "    opts, args = parser.parse_args()\n",
      "\n",
      "    if len(args) < args_min_length:\n",
      "        parser.print_help()\n",
      "        raise SystemExit, 1\n",
      "\n",
      "    return opts, args\n",
      "\n",
      "def main():\n",
      "    opts, args = parse_options(ARGS_MIN_LENGTH)\n",
      "    \n",
      "    urls_inf = args[0]\n",
      "    target_dir = args[1]\n",
      "    thread_num = int(args[2])\n",
      "    thread_delay = int(args[3])\n",
      "    socket_timeout = int(args[4])\n",
      "    \n",
      "    #set global timeout of socket\n",
      "    socket.setdefaulttimeout(socket_timeout)\n",
      "    \n",
      "    splite = ['=']*20\n",
      "    splite[10] = 'START'\n",
      "    start_time = time.time()\n",
      "    print ''.join(splite)\n",
      "\n",
      "    q = Queue.Queue()\n",
      "    with open(urls_inf, 'r') as urls:\n",
      "        for url in urls:\n",
      "            q.put(url)\n",
      "\n",
      "            \n",
      "    threads = []\n",
      "    # multiple thread\n",
      "    for i in range(thread_num):\n",
      "        t = CrawlCount(q, target_dir, thread_delay)\n",
      "        t.start()\n",
      "        threads.append(t)\n",
      "    \n",
      "    for t in threads:\n",
      "        t.join()\n",
      "    \n",
      "    end_time = time.time()\n",
      "    due = end_time-start_time\n",
      "    print 'Used time: %s' % due\n",
      "    splite[10] = 'END'\n",
      "    print ''.join(splite)\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SystemExit",
       "evalue": "2",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Usage: -c [options] <urls_input_file> <target_dir> <thread_num> <thread_delay> <timeout>\n",
        "\n",
        "-c: error: no such option: -f\n",
        "To exit: use 'exit', 'quit', or Ctrl-D."
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}